{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0 torch==2.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NnUxndVuxpzm",
        "outputId": "66133e9b-905a-4bd6-9a79-2071d7f3274a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.41.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvfuser",
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.cli.download import download\n",
        "download(model=\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n08jwJ31Fp8",
        "outputId": "83f03bcf-4588-4f01-ebec-fe8cd2aec75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fnHzBZyxErz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data import Field, LabelField, TabularDataset, BucketIterator\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "TEXT = Field(spacy_en.tokenizer, lower=True, include_lengths=True)\n",
        "LABEL = LabelField(dtype=torch.float)\n",
        "\n",
        "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# Create iterators for batching\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data), batch_size=64, device=device, sort_within_batch=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.embedding(text)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.2\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = SentimentLSTM(embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx)\n",
        "model = model.to(device)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for batch in iterator:\n",
        "        text, text_lengths = batch.text\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "N_EPOCHS = 50\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "# Evaluate the model\n",
        "def test_model(model, iterator):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "            y_true.extend(batch.label.cpu().numpy())\n",
        "            y_pred.extend(rounded_preds.cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "y_true, y_pred = test_model(model, test_iterator)\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU8VsGt53l97",
        "outputId": "81f158ef-faad-402b-875f-8250d9b77ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.660 | Train Acc: 59.92%\n",
            "\t Val. Loss: 0.573 |  Val. Acc: 71.19%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.469 | Train Acc: 78.27%\n",
            "\t Val. Loss: 0.391 |  Val. Acc: 82.42%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.381 | Train Acc: 83.13%\n",
            "\t Val. Loss: 0.350 |  Val. Acc: 84.57%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.339 | Train Acc: 85.28%\n",
            "\t Val. Loss: 0.335 |  Val. Acc: 85.35%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.308 | Train Acc: 86.89%\n",
            "\t Val. Loss: 0.315 |  Val. Acc: 86.26%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.287 | Train Acc: 87.93%\n",
            "\t Val. Loss: 0.302 |  Val. Acc: 86.91%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.259 | Train Acc: 89.29%\n",
            "\t Val. Loss: 0.306 |  Val. Acc: 87.45%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.230 | Train Acc: 90.82%\n",
            "\t Val. Loss: 0.314 |  Val. Acc: 86.84%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.206 | Train Acc: 92.07%\n",
            "\t Val. Loss: 0.308 |  Val. Acc: 87.39%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.175 | Train Acc: 93.29%\n",
            "\t Val. Loss: 0.333 |  Val. Acc: 86.62%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.145 | Train Acc: 94.66%\n",
            "\t Val. Loss: 0.362 |  Val. Acc: 87.07%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.120 | Train Acc: 95.79%\n",
            "\t Val. Loss: 0.381 |  Val. Acc: 87.28%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.094 | Train Acc: 96.67%\n",
            "\t Val. Loss: 0.448 |  Val. Acc: 86.97%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.076 | Train Acc: 97.48%\n",
            "\t Val. Loss: 0.493 |  Val. Acc: 85.80%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.060 | Train Acc: 98.07%\n",
            "\t Val. Loss: 0.464 |  Val. Acc: 85.81%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.054 | Train Acc: 98.26%\n",
            "\t Val. Loss: 0.514 |  Val. Acc: 86.84%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.047 | Train Acc: 98.39%\n",
            "\t Val. Loss: 0.509 |  Val. Acc: 85.78%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.037 | Train Acc: 98.83%\n",
            "\t Val. Loss: 0.630 |  Val. Acc: 86.40%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.038 | Train Acc: 98.66%\n",
            "\t Val. Loss: 0.593 |  Val. Acc: 85.22%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.032 | Train Acc: 98.98%\n",
            "\t Val. Loss: 0.596 |  Val. Acc: 86.20%\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.025 | Train Acc: 99.13%\n",
            "\t Val. Loss: 0.630 |  Val. Acc: 86.14%\n",
            "Epoch: 22\n",
            "\tTrain Loss: 0.029 | Train Acc: 98.97%\n",
            "\t Val. Loss: 0.708 |  Val. Acc: 86.26%\n",
            "Epoch: 23\n",
            "\tTrain Loss: 0.025 | Train Acc: 99.18%\n",
            "\t Val. Loss: 0.726 |  Val. Acc: 86.36%\n",
            "Epoch: 24\n",
            "\tTrain Loss: 0.017 | Train Acc: 99.43%\n",
            "\t Val. Loss: 0.791 |  Val. Acc: 85.59%\n",
            "Epoch: 25\n",
            "\tTrain Loss: 0.021 | Train Acc: 99.29%\n",
            "\t Val. Loss: 0.733 |  Val. Acc: 86.88%\n",
            "Epoch: 26\n",
            "\tTrain Loss: 0.012 | Train Acc: 99.60%\n",
            "\t Val. Loss: 0.825 |  Val. Acc: 86.45%\n",
            "Epoch: 27\n",
            "\tTrain Loss: 0.020 | Train Acc: 99.34%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 86.60%\n",
            "Epoch: 28\n",
            "\tTrain Loss: 0.016 | Train Acc: 99.48%\n",
            "\t Val. Loss: 0.768 |  Val. Acc: 85.75%\n",
            "Epoch: 29\n",
            "\tTrain Loss: 0.015 | Train Acc: 99.52%\n",
            "\t Val. Loss: 0.757 |  Val. Acc: 85.96%\n",
            "Epoch: 30\n",
            "\tTrain Loss: 0.014 | Train Acc: 99.57%\n",
            "\t Val. Loss: 0.676 |  Val. Acc: 86.62%\n",
            "Epoch: 31\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.54%\n",
            "\t Val. Loss: 0.785 |  Val. Acc: 86.36%\n",
            "Epoch: 32\n",
            "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
            "\t Val. Loss: 0.772 |  Val. Acc: 86.59%\n",
            "Epoch: 33\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.56%\n",
            "\t Val. Loss: 0.732 |  Val. Acc: 86.60%\n",
            "Epoch: 34\n",
            "\tTrain Loss: 0.014 | Train Acc: 99.50%\n",
            "\t Val. Loss: 0.857 |  Val. Acc: 85.95%\n",
            "Epoch: 35\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.61%\n",
            "\t Val. Loss: 0.884 |  Val. Acc: 86.81%\n",
            "Epoch: 36\n",
            "\tTrain Loss: 0.014 | Train Acc: 99.53%\n",
            "\t Val. Loss: 0.801 |  Val. Acc: 85.98%\n",
            "Epoch: 37\n",
            "\tTrain Loss: 0.011 | Train Acc: 99.66%\n",
            "\t Val. Loss: 0.867 |  Val. Acc: 86.45%\n",
            "Epoch: 38\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.73%\n",
            "\t Val. Loss: 0.928 |  Val. Acc: 85.62%\n",
            "Epoch: 39\n",
            "\tTrain Loss: 0.015 | Train Acc: 99.46%\n",
            "\t Val. Loss: 0.824 |  Val. Acc: 86.09%\n",
            "Epoch: 40\n",
            "\tTrain Loss: 0.015 | Train Acc: 99.48%\n",
            "\t Val. Loss: 0.862 |  Val. Acc: 85.62%\n",
            "Epoch: 41\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.76%\n",
            "\t Val. Loss: 0.833 |  Val. Acc: 86.87%\n",
            "Epoch: 42\n",
            "\tTrain Loss: 0.014 | Train Acc: 99.54%\n",
            "\t Val. Loss: 0.715 |  Val. Acc: 85.50%\n",
            "Epoch: 43\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.70%\n",
            "\t Val. Loss: 0.883 |  Val. Acc: 86.92%\n",
            "Epoch: 44\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.67%\n",
            "\t Val. Loss: 0.910 |  Val. Acc: 86.13%\n",
            "Epoch: 45\n",
            "\tTrain Loss: 0.010 | Train Acc: 99.63%\n",
            "\t Val. Loss: 0.830 |  Val. Acc: 86.73%\n",
            "Epoch: 46\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.71%\n",
            "\t Val. Loss: 0.890 |  Val. Acc: 86.64%\n",
            "Epoch: 47\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.76%\n",
            "\t Val. Loss: 0.806 |  Val. Acc: 86.34%\n",
            "Epoch: 48\n",
            "\tTrain Loss: 0.009 | Train Acc: 99.71%\n",
            "\t Val. Loss: 0.924 |  Val. Acc: 86.59%\n",
            "Epoch: 49\n",
            "\tTrain Loss: 0.011 | Train Acc: 99.57%\n",
            "\t Val. Loss: 0.854 |  Val. Acc: 86.19%\n",
            "Epoch: 50\n",
            "\tTrain Loss: 0.012 | Train Acc: 99.60%\n",
            "\t Val. Loss: 0.832 |  Val. Acc: 86.44%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.84      0.86     12500\n",
            "         1.0       0.85      0.89      0.87     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.87      0.86      0.86     25000\n",
            "weighted avg       0.87      0.86      0.86     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with your actual YouTube Data API key\n",
        "API_KEY = 'YOUR_API_KEY'\n",
        "\n",
        "# Create a YouTube Data API client\n",
        "youtube = build('youtube', 'v3', developerKey=API_KEY)\n"
      ],
      "metadata": {
        "id": "mQ6I4S3g5BEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_limited_video_comments(video_id, max_results=1000):\n",
        "    comments = []\n",
        "    total_fetched = 0\n",
        "\n",
        "    nextPageToken = None\n",
        "\n",
        "    while total_fetched < max_results:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            textFormat='plainText',\n",
        "            maxResults=min(100, max_results - total_fetched),  # Fetch at most 100 comments or the remaining needed\n",
        "            pageToken=nextPageToken\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "            total_fetched += 1\n",
        "\n",
        "        nextPageToken = response.get('nextPageToken')\n",
        "\n",
        "        if not nextPageToken:\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # https://www.youtube.com/watch?v=VuNIsY6JdUw\n",
        "    youTube_link = input('Enter YouTube video link: ')\n",
        "    video_id = youTube_link.split('=')[1]\n",
        "    # video_id = 'VuNIsY6JdUw'  # Replace with the actual video ID\n",
        "    max_results = 10000  # Maximum number of comments to fetch\n",
        "\n",
        "    video_comments = fetch_limited_video_comments(video_id, max_results)\n",
        "    print(\"Total comments: \", len(video_comments))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKZ-az8dDN0y",
        "outputId": "2c529f2c-f7fc-4c07-a376-a4016f5caf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter YouTube video link: https://www.youtube.com/watch?v=tSbScjc0cIk\n",
            "Total comments:  115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, sentence, TEXT, device):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in spacy_en.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()\n",
        "\n",
        "# Input sentence for testing\n",
        "test_sentence = \"I don't like this movie.\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "prediction = predict_sentiment(model, test_sentence, TEXT, device)\n",
        "\n",
        "if prediction >= 0.5:\n",
        "    print(\"Positive sentiment\")\n",
        "else:\n",
        "    print(\"Negative sentiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq-j0bozP8O1",
        "outputId": "da4d71bd-6030-4304-fc7b-e45b5c910461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive = 0\n",
        "negative = 0\n",
        "\n",
        "for comment in video_comments:\n",
        "    if comment:\n",
        "      prediction = predict_sentiment(model, comment, TEXT, device)\n",
        "      if prediction >= 0.5:\n",
        "          print('Positive: ', comment)\n",
        "          positive += 1\n",
        "      else:\n",
        "          print('Negative: ', comment)\n",
        "          negative += 1\n",
        "\n",
        "print(positive, negative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgVizDAw5ZJc",
        "outputId": "8d625ad4-c07a-4493-af5a-392e1087bc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative:  This is supposed to be a bad album? I love it!\n",
            "Positive:  Rather an interesting than a good experiment. Lots of good elements spread all over the album for sure, but somehow collectively, it's not the magic formula that convinces me fully.\n",
            "Positive:  John Bonham on Drums.\n",
            "Negative:  Essential stuff, will never be in your constant rotation. Yet, you'll be more complete as a rocker. Ssh, some consider it better than Stooges, Velvet Underground & MC5, LOL ;)\n",
            "Negative:  bruh this is killer\n",
            "Positive:  owned the original\n",
            "Negative:  insane !!!\n",
            "Negative:  Bought this when it came out   just awful\n",
            "Positive:  Listen to anything by Danzig and you’ll hear how Lord Sutch’s vocals were far ahead of their time.\n",
            "Positive:  He sang In the first track with jimmy page u can’t go wrong that’s fn amazing this beat almost could pass for Achilles Last stand\n",
            "Positive:  No doubt jimmy page abs bonzo playing on the first track toss is awesome\n",
            "Positive:  It’s typical Lord Sutch and the music is typical what was being played in the mid 60s except it’s more raunchy because it sounds like a live messy jam.\n",
            "Positive:  This is the birth of punk rock right here.\n",
            "Positive:  truly a cult album.  i remember a guy in the neighborhood had it when it first came out. sounds better today.  all the raw unprocessed creativity of guys that would go on to become legends.  a lot of cool licks.\n",
            "Negative:  Page and Beck. Great. Sutch is an awful vocalist.\n",
            "Positive:  Sutch still the ripper of vocals \n",
            "Gud brother rest easy\n",
            "Negative:  This is my first time listening to this album and I think it’s pretty cool. I have no clue as to why it’s supposedly considered to be one of the worst albums ever.\n",
            "Positive:  Its like  SF bands jamming together ; if you're expecting a finished product ; then its not good. But if you don't mind great players, playing off the cuff then its just fine.... But,  yeah, Screamin Lord Suck\n",
            "Positive:  Dans Logan Lucky de Soderbergh il y a des extraits de music top et un passage de flashing ligths, détonnant !!👍\n",
            "Negative:  Without American\" Screamin Jay\" Hawkins   , no  Lord Sutch,Crazy World of Arthur Brown, KISS , Alice Cooper ,GWAR,Slipknot... etc.\n",
            "Positive:  Page playing is worth gold and I love Bonhams intro on \"smoke and fire\".This LP and his singing is so honest that it touch my heart.\n",
            "Negative:  I am shocked I never heard this before. The singing is literally (terrible) like the singing of punk rockers who also sing terribly. The difference besides the fact that it came long before punk is that the instrumentation is awesome! I'll take it over any of that garbage any day. At least he surrounded himself with great musicians. It would be like me recruiting a great band and singing. If I sing, people die! 🤣\n",
            "Positive:  I think I picked this up on 8 track cassette when I was in high school . My friends thought it was awful but I listened to it so much I can still remember the words.\n",
            "Positive:  I remember when I first got this album, I thought it was going to be some heavy blues in the vein of John Mayall's Bluesbrakers or Peter Green's Fleetwood Mack because of who appeared on it. Imagine my surprise to find out this was a PUNK ROCK ALBUM! After the first song I exclaimed, SO THAT'S WHERE THE SEX PISTOLS GOT IT FROM! I always wondered how Jimmy Page and Jeff Beck felt about helping to invent punk rock without knowing it. Still one of my favorite roots of punk albums right up there with Death' first album.\n",
            "Negative:  people talk about the awful singing and banal lyrics as if the archies weren't the biggest hit the year before (banal lyrics) and people had never heard bob dylan (bad singing)\n",
            "Positive:  One thing I like about the lyrics and the singing: Lord Sutch is literally just screaming his heart out about being a rock n’ roll fan! “Wailing Sounds”, “Gutty Guitar”, “Thumping Beat”, they’re all referring to the very music his “heavy friends” are playing! Love the parts where he name drops Jeff Beck and Jimmy Page. A very fun listen that still holds up, especially as proto-punk\n",
            "Negative:  In den frühen 80ern kam Sutch nach Hamburg und machte die Markthalle unsicher. Hahahahahahahaha...\n",
            "Negative:  Wow... Absolutely AWFUL Music\n",
            "Negative:  Some critics call this one of the worst albums ever. I don't really understand why. It's not great by any means. It's decent. But worst album ever? Absolutely not.\n",
            "Positive:  I think Sutch was the first UK Punk. Pistols be damned! I’m sure that John Lyden might even agree!\n",
            "Positive:  Great record! Great line-up!\n",
            "Positive:  When I was a little kid in the early 70s I think I’ve seen this record and just about everybody’s house that I ever went to …just about!\n",
            "Positive:  Awful album he was out of his depth.\n",
            "Positive:  This was the best thing ever for me 🤣\n",
            "Positive:  Ho appena ascoltato questo disco per la prima volta, sono arrivato a sentirlo perché uno dei brani fa parte della colonna sonora di un film. Sono entusiasta di questo disco, bellissimo, grande energia, chitarrista favoloso, voci sporche, si sente il blues, il punk, forse gli Skiantos ascoltavano questo disco.😁 ogni volta che inizia un nuovo brano pensi che parte male e che ormai non possono più suonare niente di interessante, invece non annoiano mai! Grandi! Vorrei andare ad una festa dove suonano cosi! 👏👏👏👍\n",
            "Negative:  incredibly raw and shit.... good for pissing of the neighbors.... seems like a bunch of alcho's and junkies hung out with a garage singer who can't sing but has a good supply... yes i think they're right. it's one of the worst albums ever made, but it is listenable if you're in the mood for that sort of release.\n",
            "Negative:  Side one\n",
            "\"Wailing Sounds\" (Jimmy Page, Sutch) – 2:38\n",
            "\"'Cause I Love You\" (John Bonham, Deniel Edwards, Jimmy Page, Sutch) – 2:46\n",
            "\"Flashing Lights\" (Jimmy Page, Sutch) – 3:14\n",
            "\"Gutty Guitar\" (Sutch) – 2:33\n",
            "\"Would You Believe\" (Jay Cee) – 3:20\n",
            "\"Smoke and Fire\" (Sutch) – 2:38\n",
            "Side two\n",
            "\"Thumping Beat\" (Jimmy Page, Sutch) – 3:07\n",
            "\"Union Jack Car\" (Jimmy Page, Sutch) – 3:03\n",
            "\"One for You, Baby\" (Sutch) – 2:44\n",
            "\"L-O-N-D-O-N\" (Sutch) – 2:56\n",
            "\"Brightest Light\" (Jay Cee, Sutch) – 3:57\n",
            "\"Baby, Come Back\" (Jimmy Page, Sutch) – 2:31\n",
            "Negative:  You can hear how a lot of the ideas were jelling here between Page, and Bonham, pre-Zeppelin. Lord Sutch and Heavy Friends is the debut album of English rock singer Screaming Lord Sutch. Recording began in May 1969 at Mystic Studios in Hollywood and it was released on Cotillion Records in 1970. The album featured an all-star line-up with contributions from Led Zeppelin's Jimmy Page (who also produced the album) and John Bonham, guitarist Jeff Beck, session keyboardist Nicky Hopkins, session guitarist Deniel Edwards and Jimi Hendrix Experience bassist Noel Redding. Rick Brown and Carlo Little were previously with the Savages.\n",
            "\n",
            "This album has also been released under the name Smoke and Fire. A CD release under the latter name appeared on the Magnum Music Group label sometime in the 1980s, evidently mastered from a vinyl copy of the original album. The cover credits Page, Beck, Bonham, Redding and Hopkins; Sutch's name appears only in the personnel list, as David Sutch.\n",
            "\n",
            "Many of the musicians had grave misgivings upon its release. They were under the assumption these were demo quality recordings. As a result the artists disowned the project and the album sold poorly. It also seriously damaged Sutch's reputation with the musicians involved. \"I just went down to have a laugh, playing some old rock 'n' roll, a bit of a send-up. The whole joke sort of reversed itself and became ugly,\" Jimmy Page said of the record.\n",
            "\n",
            "A reviewer in Rolling Stone called Sutch \"absolutely terrible\" and lamented that the celebrated musicians involved were made to sound \"like a fouled parody of themselves\".\n",
            "\n",
            "Lord Sutch and Heavy Friends was named in a 1998 BBC poll as the worst album of all time, a status it also held in Colin Larkin's All Time Top 1000 Albums 3rd Edition (2000).\n",
            "\n",
            "In spite of what the critics say, I thought this album was a real find when I found it in the deleted record bins, back in my hometown of Winniipeg Manitoba.\n",
            "\n",
            "Flashing Lights is a lot of Bo Diddley.. Nicky Hopkinsis one of the best rock blues pianists. How could anything with both Beck and Page on it be bad?\n",
            "Positive:  \"Smoke & Fire\" is awesome & \"Thumping Beat\" could have been on any Zeppelin album...\n",
            "Negative:  Not as bad as some would have you believe. It all just sounds very unpolished, like demo versions.\n",
            "Positive:  Lord Sucks is more like it. Get real, without this all star line-up no one would have bought this\n",
            "Positive:  Who was the producer?\n",
            "Positive:  👏\n",
            "Positive:  that. fucking. bass. pedal.\n",
            "Negative:  Sounds like another candidate for the growing 'Punk before Punk' category to me.\n",
            "Positive:  Thumpin beat sounds like Led Zeppelin to me\n",
            "Negative:  sounds very punk/garage...doesn't sound like it was even close to being mastered..but I dig it\n",
            "Negative:  Flame on!!\n",
            "Negative:  Hey, I've definitely heard worse.  MUCH worse.  For example, an album  called \"The Purple Gang Strikes,\" which languishes in well deserved obscurity.\n",
            "Positive:  Muito foda\n",
            "Negative:  Agreed, the awful singing and clumsy lyrics are part of the charm as this album grows on you.  The line up is stellar and does not disappoint in their performance.  A  must have recording for the serious blues rock guitarist.\n",
            "Positive:  If this was released in 1977, it would've been hailed as a masterpiece.\n",
            "Positive:  Holy cow, this is a lost jem.  This is about as loose a lot of these guys ever got--and Jimmy Page and John Bonham had no problem getting loose.  It's proto-punk\n",
            "Positive:  F**k me this is satisfyingly heavy!!!\n",
            "Negative:  How is this bad? Because Page said so?\n",
            "Negative:  12:23 Jeff Beck is in the house\n",
            "Positive:  3:54 doesn’t sound like Jimmy\n",
            "Positive:  I don’t understand why critics say „its worst album” ?\n",
            "Its realy great...\n",
            "Positive:  I agree with most of you. This album kicks ass!!! I bought it the week it came out along with the first Rare Bird album. I think it's better than Zep and the studio unknowns on the album are good as well. The critics didn't like it because it's really three bands with Screaming Lord Sutch singing in all. Really just a great recording session with some fun riffs. It's not \"Tommy\" and it wasn't supposed to be.\n",
            "Positive:  Trout Metal Replica.\n",
            "Positive:  Still one of my favorite albums. Lots of energy and great rock guitar playing.\n",
            "Positive:  Jimmy Page is great.  He played guitar with folks are varied as Screaming Lord Sutch and Francoise Hardy.  Did session work with The Kinks and he produced the 1st Nico and Dana Gillespie singles.   Led Zeppelin?  Meh....\n",
            "Positive:  The backing band is f****** AMAZING!!!!\n",
            "Negative:  Ritchie Blackmore on these recordings?\n",
            "Negative:  how the hell is this bad. they should try living in 2020 for a minute or two...this the shaggs and Atila are on the worst of time list. these people are the worst...\n",
            "Negative:  enorme... Alice Cooper is a copy....\n",
            "Positive:  Definitely sounds like the Stooges. One of the last songs sounds like Ozzie. When this first came there weren’t that many heavy guitar albums Thumping Beat could be a Zeppelin song maybe Jeff Beck in there. I really loved this as a kid. Sounds punk\n",
            "Positive:  man... the music is awesome, But Lord Sutch fuckin sucks as a singer.\n",
            "Negative:  Jimmy Page & John Bonham distanced themselves from ties with this album, can see why.\n",
            "Positive:  You can't deny the elements are all there from one of the greatest collections of English rock musicians (it just needs Eric Clapton)--except the singing. I just wish Lord Such would stop Screaming and let me listen to the other artists play. It's like trying to get a selfie with the 1966 England World Cup soccer team but some extra guy from Austin Powers keeps photo-bombing me.\n",
            "Negative:  Might not be up there with JS Bach, but it beats Miley Cyrus.\n",
            "Negative:  So....what are the complaints here?  This is, at it’s absolute worst, generic Psychedelic Heavy Blues.  Otherwise I don’t see why this shouldn’t be considered a cult classic.  His voice and campy lyrics are the least great aspects here but honestly not HORRIBLE.\n",
            "Negative:  Nothing wrong with this album - he's not even screaming (try listening to some metal bands where the singer sounds like he's having parts of his anatomy tortured - sorry metalheads).  To Jimmy Page who tried to distance himself as far as possible from it I have two words: 'Cherry Cherry' - the raw demo that became an absolute stormer for Neil Diamond and which sounded like crap when produced 'professionally' with drums later on.\n",
            "Positive:  Never believe a critic!\n",
            "Negative:  Great album! Thanks for sharing 👍\n",
            "Negative:  Thats not too bad. Whats the problem?\n",
            "0:00\n",
            "2:34\n",
            "5:16\n",
            "8:22\n",
            "10:55\n",
            "14:13\n",
            "16:49\n",
            "19:57\n",
            "23:02\n",
            "25:43\n",
            "28:35\n",
            "12, Baby, Come Back 32:30\n",
            "Negative:  excuse me, ... very discerning.\n",
            "Positive:  i cannot believe only 368 likes, anyway guess im just a discerning oldhead.\n",
            "Negative:  Singer: Bad\n",
            "Music: Good\n",
            "Like he says in the song with Jimmy Page you can't go wrong.\n",
            "Negative:  I search worst song ever award and I found a wikipedia site and this song was on the site\n",
            "Positive:  There's moments in this that remind me of MC5 or The Damned... Love it.\n",
            "Parts are so of its time.... Its iconic.\n",
            "Worst album of all time? Bollocks!\n",
            "Have you listened to modern music??\n",
            "Negative:  'flashing lights'  is \"pretty thing\" by bo didley  jacked up with a different groove\n",
            "Negative:  This is gotta be a joke, wow\n",
            "Positive:  So eles na jam delirante tks\n",
            "Positive:  Great!\n",
            "Appears to be edited by Line Record label in France!\n",
            "Absolutely genius!\n",
            "John Bonham is the boss...!\n",
            "🤗\n",
            "Positive:  Insanely good guitar from Jimmy Page and great punk vocal performance from Lord Sutch!!!!\n",
            "Negative:  FIRE!!!\n",
            "Positive:  Flashing Light sounds like White Light/White Heat.\n",
            "Negative:  Compared to other bands nowadays, I have to say, not bad. Not bad at all.\n",
            "Positive:  Screaming Lord Sutch - Lead vocals\n",
            "Jimmy Page – Acoustic and electric guitar (on tracks 1-3, 5, 7, 8, 11, 12),[6] backing vocals, producer\n",
            "Jeff Beck - Electric guitar (on tracks 4, 5, 11)\n",
            "John Bonham – Drums (on tracks 1-3, 7, 8, 11, 12), percussion, backing vocals\n",
            "Nicky Hopkins - Piano, keyboards (on tracks 4, 5, 11)\n",
            "Kent Henry - Guitar (on tracks 5, 6, 9-11)\n",
            "Noel Redding - Bass guitar (on tracks 7, 9-11)\n",
            "Rick Brown - Bass guitar (on tracks 4, 5, 11)\n",
            "Deniel Edwards - Lead guitar and Bass guitar (1-3, 5, 6, 8, 11, 12)\n",
            "Martin Kohl - Bass guitar (on tracks 5, 6, 9-11)\n",
            "Carlo Little - Drums (on tracks 4-6)\n",
            "Bob Metke - Drums (on tracks 6, 9, 10)\n",
            "Tommy Caccetta - Engineer\n",
            "Negative:  Lord Sutch i love it!\n",
            "Positive:  Meanwhile, a music critic somewhere is trying to convince someone that \"Trout Mask Replica\" is brilliant...\n",
            "Negative:  how is this bad?????\n",
            "Positive:  Still a better album than In the Aeroplane Over the Sea.\n",
            "Positive:  The guy who was dressed up like a union jack from Get of of my Cloud was Lord David Sutch.\n",
            "Positive:  Beautiful limosine jhon Bonhan live\n",
            "Negative:  No Zep without SLS. Also, apparently no Stooges...\n",
            "Positive:  What A Great Album  \n",
            "2 /26 / 19\n",
            "Negative:  Wailing Sounds has a touch of Communication Breakdown about it...I wonder why he was never asked to sing for Led Zeppelin? I can't see why this was voted as the worst album ever?! Really? The worst??!!\n",
            "Negative:  Love this album, who cares what the critics say.\n",
            "Negative:  'With Jimmy Page you can't go wrong!'\n",
            "Negative:  For some the worst album ever.\n",
            "Positive:  Hard Classic\n",
            "Negative:  hahaha weww !!!!!\n",
            "Positive:  Saw Sutch live as a young teen, (early 70's) Was fabulous show, the coffin entry, fire torches, gigantic axes and swords, unique for the day. Brilliant.\n",
            "Negative:  Far out, man !!\n",
            "Negative:  thats the sound...  i really dig this album\n",
            "Positive:  englands answer to Americas wayne Cochran and the cc riders.thanks lord for lord\n",
            "Negative:  I get it! This is like Miles Davis's In A Silent Way, but for the rock folk.\n",
            "Negative:  very very nice!\n",
            "original mad man\n",
            "Negative:  good stuff but bad copy, very tinny😐\n",
            "Negative:  Wow....been meaning to listen to this for decades.....Thanks M.W.\n",
            "Positive:  Great stuff from the late 60s.  Famous names on this album including Jimmy Page on guitar.\n",
            "Positive:  Monster Raving Groovy.\n",
            "61 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('positivity: ', (positive)/(positive + negative))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua3pJA0l5sDV",
        "outputId": "319e4eed-5d13-40b5-b39b-c9eafeb9916d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positivity:  0.5304347826086957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drqqpmxnPHAX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}